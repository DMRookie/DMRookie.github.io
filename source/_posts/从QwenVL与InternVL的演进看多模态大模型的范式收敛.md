---
title: 从QwenVL与InternVL的演进看多模态大模型的范式收敛
date: 2025-10-16 09:23:53
tags:大模型 多模态
---
本文以 QwenVL 和 InternVL 系列模型为研究对象，探讨多模态大模型的演进路径，重点关注模型架构与训练流程两个维度。通过梳理这两个系列的迭代过程，可以总结出以下关键趋势：

在模型架构方面，主流设计逐渐收敛于“VIT-MLP-LLM”的三级架构范式，即视觉编码器（VIT）、轻量级适配层（MLP）与大语言模型（LLM）依次连接，结构趋于统一。

在训练流程上，多阶段训练策略成为共识，通常遵循“预训练—指令微调—强化学习”的递进流程。其中，预训练阶段的策略有明显演进：早期方法通常冻结语言模型，仅训练视觉编码器与适配层，随后再解冻语言模型进行整体微调；而近期方法则倾向于直接进行全局参数联合训练，以提升数据利用效率和模型性能。

此外，两个系列均广泛采用动态分辨率机制，在输入阶段尽可能保持图像原始比例，减少形变，从而提升视觉特征的提取质量。在数据层面，大规模、高质量的数据构建也成为关键方向，包括严格的真实数据清洗与高效的合成数据生成，共同推动模型性能的提升。

## QwenVL系列
### QwenVL 1
QwenVL1 较为简单，语言解码层只用了一个7.7B规格的Qwen1做初始化，视觉编码层用了一个1.9B的VIT，用VIT-bigG做初始化。中间适配层用的CrossAtten。QwenVL1只处理单图像数据。预处理时，会把图像统一缩放到固定大小（224/448）。经过适配层后，将图片特征压缩到一个固定长度，如256。

训练流程分三步：预训练时使用大规模，弱标注的图像文本对数据。冻结语言层，训练视觉编码层和适配层。多任务预训练时，使用高质量，细粒度标注的数据。涵盖7个任务，以提升模型的多任务能力。这一步会训练整个模型。最后一步有监督微调，使用对话数据，提升模型指令遵循和对话能力。在这步会冻结视觉编码层，训练语言层和适配层。

![image.png](https://km.woa.com/asset/000100022510000f2eae16301d4e9501?height=1878&width=3344)

### QwenVL 2
QwenVL2的视觉编码层是用一个675M大小的VIT，使用DFN模型做初始化。适配层改为简单的两层MLP层，语言层用了1.5/7/72B三个规格的Qwen2做初始化。预处理时，预定义一组长宽比，将图片缩放到最匹配的长宽比。改进点包括：
1. **朴素动态分辨率**: 引入了朴素动态分辨率机制，使模型能够动态地将不同分辨率的图像转换为不同数量的视觉令牌。为了支持这一特性，修改了VIT，去除了原始的绝对位置嵌入，并引入了2D旋转位置嵌入（2D-RoPE）来捕捉图像的二维位置信息。在推理阶段，不同分辨率的图像被打包成一个序列，并通过一个简单的MLP层压缩相邻的2×2令牌为一个令牌。
2. ** 多模态旋转位置嵌入**: 引入了多模态旋转位置嵌入，以有效建模多模态输入的位置信息。M-RoPE将原始的旋转嵌入解构为三个组件：时间、高度和宽度。对于文本输入，这三个组件使用相同的ID，使其在功能上等同于1D-RoPE。对于图像，每个视觉令牌的时间ID保持不变，而高度和宽度组件根据令牌在图像中的位置分配不同的ID。对于视频，每个帧的时间ID递增，高度和宽度组件遵循与图像相同的ID分配模式。
3. **统一的图像和视频理解**: Qwen2-VL在训练阶段增加了多图和视频数据，以确保模型在图像理解和视频理解方面的全面能力。为了尽可能保留视频信息，每个视频以每秒两帧的速度采样。其次，集成了深度为2的三维卷积来处理视频输入，估计是把同一秒的两帧合并处理。然后再动态调整视频帧的分辨率，使每个视频的token数不超过16384。最后为保持一致，将每张图片当成两个完全一样的帧进行处理。

训练流程与QwenVL1一致，分为3个步骤。视觉层预训练、多任务预训练以及有监督微调。不同点是使用更多类型的数据，包括视频数据，Agent相关数据。同时也支持更多国家的语言，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。

![image.png](https://km.woa.com/asset/000100022510006abae23dd363402e01?height=1880&width=3346)

### QwenVL 2.5
QwenVL2.5的语言层使用3/7/72B三个规格的Qwen2.5做初始化。视觉编码层使用一个676M大小的VIT，并从头预训练。适配层还是使用MLP层。处理过程中，会将相连两帧的视频图像做合并，图像中相连4块patch做合并。改进点包括:
1. **窗口注意力**：在视觉编码器的大部分层中，实现窗口注意力，以优化推理效率。窗口注意力确保计算成本与块的数量线性相关，而不是二次相关。
2. **动态帧率采样**：将动态分辨率扩展到时间维度，即不同视频可以有不同的fps采样率。通过适应可变帧率，模型能更好地捕捉视频内容的时间动态。
3. **时间域MRoPE对齐**：通过对齐MRoPE ID与绝对时间，促进更复杂的时间序列学习。这种方法允许模型通过时间ID之间的间隔学习一致的时间对齐。对于8秒的视频，当fps为0.5，则相邻帧的时间ID间隔为15。fps为1，时间ID间隔则为5。
4. **高质量数据预训练**：在预训练阶段，显著扩展了数据集规模，从1.2万亿标记增加到约4.1万亿标记。数据集包括图像字幕、交错图像文本数据、光学字符识别（OCR）数据、视觉知识、多模态学术问题、定位数据、文档解析数据、视频描述、视频定位和基于代理的交互数据。

训练流程分为4个步骤，前两个步骤与之前一致。第三步是专门针对长上下文进行预训练，将序列拉长至32k，整个模型都打开进行训练。最后一步增加了DPO强化训练，利用偏好数据来使模型与人类偏好保持一致。
![image.png](https://km.woa.com/asset/00010002251000b5404d62ae92438401?height=1882&width=3344)
### QwenVL 3
QwenVL 3详细论文还没出，从博客上列出的改进点包括:
1. **文本视觉协同预训练**：Qwen3-VL 在预训练早期即混合文本与视觉模态协同训练，文本能力持续强化，最终在纯文本任务上表现与 Qwen3-235B-A22B-2507 纯文本旗舰模型不相上下 。
2. **MRoPE-Interleave**：在 Qwen3-VL 中采取了 t,h,w 交错分布的形式，实现对时间，高度和宽度的全频率覆盖，提升对长视频的理解能力。
3. **DeepStack** ： 融合 ViT 多层次特征，提升视觉细节捕捉能力和图文对齐精度。
4. **文本时间戳对齐机制**： 采用“时间戳-视频帧”交错的输入形式，实现帧级别的时间信息与视觉内容的细粒度对齐。同时，模型原生支持“秒数”与“时:分:秒”（HMS）两种时间输出格式。

## InternVL系列
### InternVL 1
InternVL1的架构稍微复杂，预训练阶段的语言层用7B的LLaMA进行初始化，SFT阶段改为13B的Vicuna进行初始化。视觉编码层用6B的InternVIT，并从头预训练。同样只处理单图，并将图片统一缩放到固定大小。中间适配层用的CrossAtten和MLP。

训练流程分三步，第一步参考CLIP，同时打开语言层和视觉编码层，进行对比学习。第二步，冻住语言层和视觉层，训练中间的CrossAttn适配层。第三步进行有监督微调，有两种实现方案：一种是直接用MLP层连接语言层和视觉层，对MLP层进行预训练。另外一种是将第二步的QLLaMA接进去，增加一个MLP层进行预训练。其中视觉层冻住，语言层打开或冻住都可以。

![image.png](https://km.woa.com/asset/0001000225100019b422d0876643b201?height=1880&width=3346)
### InternVL 1.5
InternVL 1.5在结构上进行了简化，语言层用20B的InternLM2进行初始化。视觉层还是6B的InternVIT，适配层用的MLP层。仍然只处理单图片数据。预处理时，将图片缩放到最匹配的预定义长宽比，并切成448\*448大小的patch。同时将全图缩放到448\*448的缩略图，一起做为视觉层的输入。经过VIT后，用pixel unshuffle进行采样，减少视觉token的个数。 改进点包括:
1. **动态高分辨率**：根据输入图像的宽高比和分辨率，将图像分割成从1到40个448x448像素的瓦片，支持高达4K分辨率的输入。
2. **强大的视觉编码器**：探索了一种针对大规模视觉基础模型InternViT-6B的连续学习策略，提升其视觉理解能力，并使其能够在不同大型语言模型之间转移和复用。
4. **高质量双语数据集**：精心收集了一个高质量的双语数据集，涵盖了常见场景、文档图像，并用中英文问答对进行了注释，显著提升了OCR和与中文相关的任务性能。

训练流程主要包含两步，第一步预训练，会冻结语言层，对视觉层和适配层进行训练。这其中又分两小步。1）去掉InternVIT6B后三层，并且与ous-Hermes-2-Yi-34B语言模型做整合，然后用固定的448\*448 图像进行训练。2）改用动态的448\*448 patch进行训练，并增大训练数据量级、质量和多样性。第二步有监督微调，语言层改回InternLM2，并打开整个模型进行微调。

![image.png](https://km.woa.com/asset/00010002251000570ebf0339cd4d9c01?height=1880&width=3348)
### InternVL 2.5
InternVL 2.5的语言层分别用了InternLM2.5和Qwen2.5，并训了0.5B到72B多种大小的规格。适配层用的MLP层。视觉层用了300M和6B两种规格的InternVIT。训练数据引入了多图和视频。改进点包括:
1. **渐进式扩展**：一种渐进式扩展策略，以有效地使视觉编码器与大型语言模型对齐(InternVL 1.5和2.0也有采用)。即先用一个小规格的语言层（如20B）和视觉层一起做大量预训练，然后将训练好的视觉层迁移到更大的语言模型（如72B），而无需重新训练，以此避免大语言模型训练时的高计算成本。这做法是源于他们的观察：使用NTP损失联合训练VIT和大型语言模型，得到的视觉特征是可泛化的，其他大型语言模型也能轻松理解。
2. **训练增强**：引入了随机JPEG压缩和损失重加权技术，以提高模型的鲁棒性和性能。随机JPEG压缩用于避免过拟合并增强模型对现实世界图像的鲁棒性，而损失重加权则通过平衡不同长度响应的贡献来提高模型性能。
3. **多模态数据打包**：通过将多个样本连接成更长的序列来减少填充，从而最大化利用模型的输入序列容量，以提高GPU的利用率并提升训练效率。

训练流程分三步，第一步是MLP层的热身(warmup)训练，这阶段冻住语言层和视觉层，只训练MLP层。数据采用结构化的ChatML格式，并用NTP损失进行优化。此外，应用更高的学习率以加速收敛。第二步是VIT增量训练，这一步会打开视觉层和MLP层一起训练，并使用和第一阶段相同的数据进行训练。同时采用较低学习率以防止灾难性遗忘。这第二步是可选的，即只在小规格的语言模型上训练，训完后直接将视觉层迁移到更大规格的语言模型上。第三步是全模型的指令调优，这一步打开整个模型，用高质量的数据进行微调。在一开始就使用对话形式进行预训练比较少见。

![image.png](https://km.woa.com/asset/000100022510000cae64d80e9c457701?height=1886&width=3346)
### InternVL 3
InternVL 3的语言层采用多种规格的Qwen2.5和InternLM3做初始化。适配层和视觉层和INternVL2.5一致。改进点包括:
1.  **原生多模态预训练**: InternVL3采用了一种统一的多模态预训练方法，通过在预训练阶段同时暴露于文本数据和多样化的多模态数据集，使模型能够同时获得语言和多模态能力。
2. **变量视觉位置编码（V2PE）**: 为了支持更长的多模态上下文，InternVL3采用了变量视觉位置编码（V2PE），使用较小的、更灵活的位置增量来处理视觉标记。
3. **后训练策略**: 采用了监督微调（SFT）和混合偏好优化（MPO）等先进的后训练技术，以及测试时间缩放策略和优化的训练基础设施。

训练流程分三步，第一步直接对整个模型进行预训练，这种统一的训练方案使预训练模型能够同时学习语言能力和多模态能力，最终增强其处理视觉-语言任务的能力，而无需引入额外的桥梁模块或后续的模型间对齐程序。数据上增加了与图形用户界面（GUI）、工具使用、3D场景理解以及视频理解任务相关的额外数据。损失函数使用NTP Loss，但只计算文本token的loss。第二步是有监督微调，相比上一版引入了更高质量和更多样化的训练数据。第三步是混合偏好优化（MPO）。通过引入来自正面和负面样本的额外监督，使模型响应分布与真实分布对齐，从而提高推理性能。MPO的训练目标是偏好损失、质量损失和生成损失三者的组合。
另外在预测时，做了测试时缩放（TTS）。通过预测N个结果，并使用VisualPRM-8B作为评估器模型来选择最佳的回应进行推理和数学评估。

![image.png](https://km.woa.com/asset/0001000225100068b0074d29a842a101?height=1880&width=3346)
### InternVL 3.5
InternVL 3.5的语言层选用多种规格的Qwen3和GPT-OSS，适配层和视觉层维持不变。新增了一个视觉分辨率路由器，将高/低分辨率的图片送往不同的Pixel Unshuffle进行压缩。改动点包括:
1. **级联强化学习（Cascade RL）**: 通过离线RL阶段实现稳定收敛，在线RL阶段精细调整输出分布，从而提高推理能力。离线RL阶段作为有效的热身，确保高质量的rollouts用于后续在线阶段。
2. **视觉分辨率路由器（ViR）**: 动态选择视觉token的最佳分辨率，减少推理成本，同时保持性能。大体是同一样本，分别用1/4压缩率和1/16压缩率分别计算损失。然后将1/16压缩率的损失除以1/4压缩率的损失得到一个比率r。滑动保存历史r，并计算k 百分位数得到一个阈值。将小于阈值的样本采用1/16压缩率，否则用1/4，得到训练集后进行二分类训练。
3. **解耦视觉-语言部署（DvD）**: 将视觉编码器和语言模型部署在不同的GPU上，平衡计算负载，最大化并行性和硬件利用率。

训练阶段分为四步，第一步打开整个模型做预训练，没有提到将损失计算仅限于文本token。第二步是有监督微调，相比InternVL3，增加了“思考”模式下的多模态推理数据和新技能数据，包括基于GUI的交互、具身交互以及可伸缩矢量图形（SVG）的理解和生成。第三步是级联的强化学习，首先使用离线RL算法对模型进行微调，作为高效的热身阶段以达到满意的结果，这可以保证为后续阶段提供高质量的回放。随后，采用在线RL算法来进一步根据模型自身生成的回放细化输出分布。最后一步是视觉一致性训练，冻住主体模型，只微调视觉分辨率路由器。

![image.png](https://km.woa.com/asset/00010002251000e96dea67e01847b701?height=1880&width=3344)
